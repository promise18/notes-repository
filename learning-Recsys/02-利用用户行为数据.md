基于用户行为分析的推荐算法是个性化推荐系统的重要算法，学术界一般将这种类型的算法称为**协同过滤**算法。顾名思义，协同过滤就是指用户可以齐心协力，通过不断地和网站互动，使自己的推荐列表能够不断过滤掉自己不感兴趣的物品，从而越来越满足自己的需求。

## 1.用户行为数据简介

用户行为数据在网站上最简单的存在形式就是日志。这些日志记录了用户的各种行为，如在电子商务网站中这些行为主要包括网页浏览、购买、点击、评分和评论等。

用户行为在个性化推荐系统中一般分两种——显式反馈行为（explicit feedback）和隐式反馈行为（implicit feedback）。

显式反馈行为包括用户明确表示对物品喜好的行为。

隐式反馈行为指的是那些不能明确反应用户喜好的行为。最具代表性的隐式反馈行为就是页面浏览行为。

![1560391614147](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560391614147.png)

按照反馈的方向分，可以分为正反馈和负反馈。

正反馈指用户的行为倾向于指用户喜欢该物品，而负反馈指用户的行为倾向于指用户不喜欢该物品。在显性反馈中，很容易区分一个用户行为是正反馈还是负反馈，而在隐性反馈行为中，就相对比较难以确定。 

![1560393913731](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560393913731.png)

互联网中的用户行为有很多种，比如浏览网页、购买商品、评论、评分等。要用一个统一的方式表示所有这些行为是比较困难的。表2-3给出了一种表示方式，它将一个用户行为表示为6部分，即产生行为的用户和行为的对象、行为的种类、产生行为的上下文、行为的内容和权重。 

![1560395012171](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560395012171.png)

**无上下文信息的隐式反馈数据集**：每一条行为记录仅仅包含用户ID和物品ID。（Book-Crossing）

**有上下文信息的显性反馈数据集** ：每一条记录包含用户ID、物品ID、用户对物品的评分和评分行为发生的时间戳 。（Netflix Prize ）

## 2.用户行为分析

### 2.1 用户活跃度和物品流行度的分布

很多关于互联网数据的研究发现，互联网上的很多数据分布都满足一种称为Power Law的分
布，这个分布在互联网领域也称**长尾分布**。 

![1560476373093](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560476373093.png)

令f~u~(k)对k个物品产生过行为的用户数，令f~i~(k)为被k个用户产生过行为的物品数。那么，如下面两张图图所示：

![1560497452013](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560497452013.png)

横坐标是物品的流行度K，纵坐标是流行度为K的物品的总数。这里，物品的流行度指对物品产生过行为的用户总数。 

![1560497478704](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560497478704.png)

横坐标是用户的活跃度K，纵坐标是活跃度为K的用户总数。这里，用户的活跃度为用户产生过行为的物品总数。 

### 2.2 用户活跃度和物品活跃度的关系

一般认为，新用户倾向于浏览热门的物品，因为他们对网站还不熟悉，只能点击首页的热门物品，而老用户会逐渐开始浏览冷门的物品。 

仅仅基于用户行为数据设计的推荐算法一般称为协同过滤算法。 

- 基于邻域的方法（neighborhood-based）
- 隐语义模型（latent factor model） 
- 基于图的随机游走算法（random walk on graph） 

## 3.基于领域的算法（协同过滤）

在这些方法中，最著名的、在业界得到最广泛应用的算法是基于邻域的方法，而基于邻域的方法主要包含下面两种算法。 

（1）**基于用户的协同过滤：**这种算法给用户推荐和他兴趣相似的其他用户喜欢的物品 。

（2）**基于物品的协同过滤算法：** 这种算法给用户推荐和他之前喜欢的物品相似的物品。

### 3.1 基于用户的协同过滤算法

#### 3.1.1 算法思想

基于用户的协同过滤算法主要包括两个步骤。 

1. 找到和目标用户兴趣相似的用户集合。
2. 找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户。 

步骤1的关键就是计算两个用户的兴趣相似度。这里，协同过滤算法主要利用行为的相似度计算兴趣的相似度。给定用户u和用户v，令N(u)表示用户u曾经有过正反馈的物品集合，令N(v)为用户v曾经有过正反馈的物品集合。那么，我们可以通过如下的Jaccard公式简单地计算u和v的兴趣相似度： 

![1560498462683](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560498462683.png)

或者通过余弦相似度计算： 

![1560498475191](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560498475191.png)

下面以图2-6中的用户行为记录为例，举例说明UserCF计算用户兴趣相似度的例子。

![1560498620562](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560498620562.png)

在该例中，用户A对物品{a, b, d}有过行为，用户B对物品{a, c}有过行为，利用余弦相似度公式计算用户A和用户B的兴趣相似度为：

![1560498633949](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560498633949.png)

同理，我们可以计算出用户A和用户C、 D的相似度： 

![1560498665251](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560498665251.png)

上面的算法将很多时间浪费在了计算这种用户之间的相似度上。

**首先，需要建立物品—用户的倒排表，即对于每个物品都保存对该物品产生过行为的用户列表（如图2-7所示）。然后，建立一个4×4的用户相似度矩阵W，对于物品a，将W\[A][B]和W\[B][A]加1，对于物品b，将W\[A][C]和W\[C][A]加1，以此类推。**

扫描完所有物品后，我们可以得到最终的W矩阵。这里的W是余弦相似度中的分子部分，然后将W除以分母可以得到最终的用户兴趣相似度。

![1560499353414](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560499353414.png)

得到用户之间的兴趣相似度后， UserCF算法会给用户推荐和他兴趣最相似的K个用户喜欢的物品。如下的公式度量了UserCF算法中用户u对物品i的感兴趣程度： 

![1560500101912](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560500101912.png)

其中， **S(u, K)包含和用户u兴趣最接近的K个用户**， **N(i)是对物品i有过行为的用户集合**， **wuv是用户u和用户v的兴趣相似度**， rvi代表用户v对物品i的兴趣，因为使用的是单一行为的隐反馈数据，所以所有的rvi=1。 

利用上述算法，可以给图2-7中的用户A进行推荐。选取K=3，用户A对物品c、 e没有过行为，根据UserCF算法，用户A对物品c、 e的兴趣是： 

![1560500272161](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560500272161.png)

因此可以把这两个物品推荐给用户A。

#### 3.1.2 用户相似度改进

计算用户兴趣相似度的最简单的公式（余弦相似度公式），但这个公式过于粗糙。

首先，以图书为例，如果两个用户都曾经买过《新华字典》，这丝毫不能说明他们兴趣相似，因为绝大多数中国人小时候都买过《新华字典》。但如果两个用户都买过《数据挖掘导论》，那可以认为他们的兴趣比较相似，因为只有研究数据挖掘的人才会买这本书。换句话说，两个用户对冷门物品采取过同样的行为更能说明他们兴趣的相似度。

因此， 根据用户行为计算用户的兴趣相似度：

![1560500597336](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560500597336.png)

**该公式通过$1\over log1+|N(i)|$ 惩罚了用户u和用户v共同兴趣列表中热门物品对他们相似度的影响。**

==基于用户的协同过滤缺点：==

1. 随着网站的用户数目越来越大，计算用户兴趣相似度矩阵将越来越困难，其运算时间复杂度和空
   间复杂度的增长和用户数的增长近似于平方关系。 
2. 基于用户的协同过滤很难对推荐结果作出解释。 

### 3.2 基于物品的协同过滤算法

基于物品的协同过滤（item-based collaborative filtering）算法是目前业界应用最多的算法。 

比如，该算法会因为你购买过《数据挖掘导论》而给你推荐《机器学习》。不过， ItemCF算法并不利用物品的内容属性计算物品之间的相似度，**它主要通过分析用户的行为记录计算物品之间的相似度**。该算法认为，物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品B。

基于物品的协同过滤算法可以利用用户的历史行为给推荐结果提供推荐解释，比如给用户推荐《天龙八部》的解释可以是因为用户之前喜欢《射雕英雄传》。 

#### 3.2.1 算法思想

基于物品的协同过滤算法主要分为两步：

1. 计算物品之间的相似度。 
2. 根据物品的相似度和用户的历史行为给用户生成推荐列表。 

定义物品的相似度：

![1560501484534](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560501484534.png)

这里，分母|N(i)|是喜欢物品i的用户数，而分子 |N(i)$\bigcap$N(j)|是同时喜欢物品i和物品j的用户数。因此，**上述公式可以理解为喜欢物品i的用户中有多少比例的用户也喜欢物品j。** 

上述公式虽然看起来很有道理，但是却存在一个问题。如果物品j很热门，很多人都喜欢，那么Wij就会很大，接近1。 为了避免推荐热门的物品，可以用下面的公式：

![1560501764668](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560501764668.png)

这个公式看起来和userCF相似度计算很像。

这个公式惩罚了物品j的权重，因此减轻了热门物品会和很多物品相似的可能性。 

从上面的定义可以看到，在协同过滤中两个物品产生相似度是因为它们共同被很多用户喜欢，也就是说每个用户都可以通过他们的历史兴趣列表给物品“贡献”相似度。==这里面蕴涵着一个假设，就是每个用户的兴趣都局限在某几个方面，因此如果两个物品属于一个用户的兴趣列表，那么这两个物品可能就属于有限的几个领域，而如果两个物品属于很多用户的兴趣列表，那么它们就可能属于同一个领域，因而有很大的相似度。==

和UserCF算法类似，**用ItemCF算法计算物品相似度时也可以首先建立用户—物品倒排表（即对每个用户建立一个包含他喜欢的物品的列表）**，然后对于每个用户，将他物品列表中的物品两两在共现矩阵C中加1。

如图2-11，图中最左边是输入的用户行为记录，每一行代表一个用户感兴趣的物品集合。然后，对于每个物品集          合，我们将里面的物品两两加一，得到一个矩阵。最终将这些矩阵相加得到上面的C矩阵。**其中C\[i][j]记录了同时喜欢物品i和物品j的用户数**。最后，将C矩阵归一化可以得到物品之间的余弦相似度矩阵W。 

![1560502811450](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560502811450.png)

**举第一个例子：用户u喜欢a,b,d三个商品，那么矩阵即为用户u喜欢a商品的同时喜欢b和d商品。**

在得到物品之间的相似度后， ItemCF通过如下公式计算用户u对一个物品j的兴趣： 

![1560508843740](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560508843740.png)

**这里N(u)是用户喜欢的物品的集合， S(j,K)是和物品j最相似的K个物品的集合， wji是物品j和i的相似度， rui是用户u对物品i的兴趣。（对于隐反馈数据集，如果用户u对物品i有过行为，即可令rui=1。）该公式的含义是，和用户历史上感兴趣的物品越相似的物品，越有可能在用户的推荐列表中获得比较高的排名。** 

图2-12是一个基于物品推荐的简单例子。该例子中，用户喜欢《C++ Primer中文版》和《编程之美》两本书。然后ItemCF会为这两本书分别找出和它们最相似的3本书，然后根据公式的定义计算用户对每本书的感兴趣程度。比如， ItemCF给用户推荐《算法导论》，是因为这本书和《C++Primer中文版》相似，相似度为0.4，而且这本书也和《编程之美》相似，相似度是0.5。考虑到用户对《C++ Primer中文版》的兴趣度是1.3，对《编程之美》的兴趣度是0.9，那么用户对《算法导论》的兴趣度就是1.3 × 0.4 + 0.9×0.5 = 0.97。 



![1560509711757](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560509711757.png)

#### 3.2.2 用户活跃度对物品相似度的影响

每个用户的兴趣列表对物品的相似度产生的贡献是不相同的。

假设有这么一个用户，他是开书店的，并且买了当当网上80%的书准备用来自己卖。 这个用户虽然活跃，但是买这些书并非都是出于自身的兴趣，而且这些书覆盖了当当网图书的很多领域，所以这个用户对于他所购买书的两两相似度的贡献应该远远小于一个只买了十几本自己喜欢的书的文学青年。 

活跃用户对物品相似的的贡献应该小于不活跃的用户，因此需要修正物品相似度的计算公式：

![1560510436057](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560510436057.png)

（和userCF改进的公式很像）

#### 3.2.3 归一化

如果将ItemCF的相似度矩阵按最大值归一化，可以提高推荐的准确率。

如果已经得到了物品相似度矩阵w，那么可以用如下公式得到归一化之后的相似度矩阵w'： 

![1560510972490](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560510972490.png)

其实，归一化的好处不仅仅在于增加推荐的准确度，它还可以提高推荐的覆盖率和多样性。

举一个例子，假设在一个电影网站中，有两种电影——纪录片和动画片。那么， ItemCF算出来的相似度一般是纪录片和纪录片的相似度或者或者动画片和动画片的相似度大于纪录片和动画片的相似度。但是纪录片之间的相似度和动画片之间的相似度却不一定相同。假设物品分为两类——A和B， A类物品之间的相似度为0.5， B类物品之间的相似度为0.6，而A类物品和B类物品之间的相似度是0.2。在这种情况下，如果一个用户喜欢了5个A类物品和5个B类物品，用ItemCF给他进行推荐，推荐的就都是B类物品，因为B类物品之间的相似度大。但如果归一化之后， A类物品之间的相似度变成了1， B类物品之间的相似度也是1，那么这种情况下，用户如果喜欢5个A类物品和5个B类物品，那么他的推荐列表中A类物品和B类物品的数目也应该是大致相等的。从这个例子可以看出，相似度的归一化可以提高推荐的多样性。

#### 3.2.4 UserCF和ItemCF比较

为什么Digg使用UserCF，而亚马逊网使用ItemCF呢？ 

回顾一下UserCF和ItemCF算法的推荐原理：

- UserCF给用户推荐那些和他有共同兴趣爱好的用户喜欢的物品。

- ItemCF给用户推荐那些和他之前喜欢的物品类似的物品。

从这个算法的原理可以看到， UserCF的推荐结果着重于反映和用户兴趣相似的小群体的热点，而ItemCF的推荐结果着重于维系用户的历史兴趣。换句话说， UserCF的推荐更社会化，反映了用户所在的小型兴趣群体中物品的热门程度，而ItemCF的推荐更加个性化，反映了用户自己的兴趣传承。



在新闻网站中，用户的兴趣不是特别细化，绝大多数用户都喜欢看热门的新闻。即使是个性化，也是比较粗粒度的，比如有些用户喜欢体育新闻，有些喜欢社会新闻，而特别细粒度的个性化一般是不存在的。 比方说，很少有用户只看某个话题的新闻，主要是因为这个话题不可能保证每天都有新的消息，而这个用户却是每天都要看新闻的。因此，个性化新闻推荐更加强调抓住新闻热点，热门程度和时效性是个性化新闻推荐的重点，而个性化相对于这两点略显次要。 

但是，在图书、电子商务和电影网站，比如亚马逊、豆瓣、 Netflix中， ItemCF则能极大地发挥优势。首先，在这些网站中，用户的兴趣是比较固定和持久的。一个技术人员可能都是在购买技术方面的书，而且他们对书的热门程度并不是那么敏感，事实上越是资深的技术人员，他们看的书就越可能不热门。此外，这些系统中的用户大都不太需要流行度来辅助他们判断一个物品的好坏，而是可以通过自己熟悉领域的知识自己判断物品的质量。因此，这些网站中个性化推荐的任务是帮助用户发现和他研究领域相关的物品。因此， ItemCF算法成为了这些网站的首选算法。此外，这些网站的物品更新速度不会特别快，一天一次更新物品相似度矩阵对它们来说不会造成太大的损失，是可以接受的。 

![1560512101602](D:\笔记\推荐系统\02-利用用户行为数据.assets\1560512101602.png)

## 4.其他算法介绍

### 1 隐语义模型（LFM）

#### 1.1 算法思想

它的核心思想是通过隐含特征联系用户兴趣和物品。

举个例子：

用户A的兴趣涉及侦探小说、科普图书以及一些计算机技术书，而用户B的兴趣比较集中在数学和机器学习方面。 

那么如何给A和B推荐图书呢？ 

有一种方法，可以对书和物品的兴趣进行分类。对于某个用户，首先得到他的兴趣分类，然后从分类中挑选他可能喜欢的物品。 

这个基于兴趣分类的方法大概需要解决3个问题： 

- 如何给物品进行分类？ 
- 如何确定用户对哪些类的物品感兴趣，以及感兴趣的程度？ 
- 对于一个给定的类，选择哪些属于这个类的物品推荐给用户，以及如何确定这些物品在一个类中的权重？ 

**如何给物品进行分类**？

隐含语义分析技术因为采取基于用户行为统计的自动聚类，因此对于**第一个问题**：

1. 编辑的意见不能代表各种用户的意见，但隐含语义分析技术的分类来自对用户行为的统计，代表了用户对物品分类的看法。**隐含语义分析技术和ItemCF在物品分类方面的思想类似，如果两个物品被很多用户同时喜欢，那么这两个物品就很有可能属于同一个类。**
2. 编辑很难控制分类的粒度，**但隐含语义分析技术允许我们指定最终有多少个分类，这个数字越大，分类的粒度就会越细，反正分类粒度就越粗。** 
3. 编辑很难给一个物品多个分类， 但**隐含语义分析技术会计算出物品属于每个类的权重，因此每个物品都不是硬性地被分到某一个类中。**
4. 编辑很难给出多维度的分类，但**隐含语义分析技术给出的每个分类都不是同一个维度的，它是基于用户的共同兴趣计算出来的**，如果用户的共同兴趣是某一个维度，那么LFM给出的类也是相同的维度。 
5. 编辑很难决定一个物品在某一个分类中的权重，但**隐含语义分析技术可以通过统计用户行为决定物品在每个类中的权重，如果喜欢某个类的用户都会喜欢某个物品，那么这个物品在这个类中的权重就可能比较高。** 

隐含语义分析技术从诞生到今天产生了很多著名的模型和方法，其中和该技术相关且耳熟能详的名词有**pLSA**、 **LDA**、**隐含类别模型（latent class model）**、**隐含主题模型（latent topic model）**、**矩阵分解（matrix factorization）**。这些技术和方法在本质上是相通的，其中很多方法都可以用于个性化推荐系统。 

**如何计算用户对物品的兴趣？**

LFM通过如下公式计算用户u对物品i的兴趣：

![1560601032170](02-利用用户行为数据.assets\1560601032170.png)

其中，p~u,k~和q~i,k~是模型的参数。p~u,k~度量了用户u的兴趣和第k个隐类的关系，q~i,k~度量了第k个隐类和物品i之间的关系。接下来是如何计算着两个参数。

这两个参数是从数据集中计算出来的。要计算这两个参数，需要一个训练集，对于每个用户u，训练集里都包含了用户u喜欢的物品和不感兴趣的物品，通过学习这个数据集，就可以获得上面的模型参数。 

在对负样本进行采样时应该遵循以下原则：

- 对每个用户，要保证正负样本的平衡（数目相似）。 
- 对每个用户采样负样本时，要选取那些很热门，而用户却没有行为的物品。 

经过采样，可以得到一个用户—物品集 K={(u, i)} ，其中如果(u, i)是正样本，则有 r~ui~=1，否则有 r~ui~= 0 。然后，需要优化如下的损失函数来找到最合适的参数p和q： 

![1560601631269](02-利用用户行为数据.assets\1560601631269.png)

这里， $\lambda$||p~u~||^2^+$\lambda$||q~i~||^2^是用来防止过拟合的正则化项， λ可以通过实验获得。要最小化上面的损失函数，可以利用一种称为**随机梯度下降法**的算法。它首先通过求参数的偏导数找到最速下降方向，然后通过迭代法不断地优化参数。 

### 2 基于图的模型

用户行为很容易用二分图表示，因此很多图的算法都可以用到推荐系统中。 

#### 2.1 用户行为数据的二分图表示

在研究基于图的模型之前，首先需要将用户行为数据表示成图的形式。本章讨论的用户行为数据是由一系列二元组组成的，其中每个二元组(u, i)表示用户u对物品i产生过行为。这种数据集很容易用一个二分图表示。 

**令G（V， E）表示用户物品二分图，其中V=V~U~ $\bigcup$ V~I~由用户顶点集合V~U~和物品顶点集V~I~组成。对于数据集中每一个二元组(u, i)，图中都有一套对应的边 e(v~u~，v~i~) ，其中 v~u~$\in$V~U~是用户u对应的顶点，v~i~$\in$V~I~是物品i对应的顶点。** 

图2-18是一个简单的用户物品二分图模型，其中圆形节点代表用户，方形节点代表物品，圆形节点和方形节点之间的边代表用户对物品的行为。比如图中用户节点A和物品节点a、 b、 d相连，说明用户A对物品a、 b、 d产生过行为。 

![1560605893418](02-利用用户行为数据.assets\1560605893418.png)

#### 2.2 基于图的推荐算法

将用户行为表示为二分图模型后，下面的任务就是在二分图上给用户进行个性化推荐。

如果将个性化推荐算法放到二分图模型上，那么给用户u推荐物品的任务就可以转化为度量用户顶点v~u~和与v~u~没有边直接相连的物品节点在图上的相关性，相关性越高的物品在推荐列表中的权重就越高。

一般来说图中顶点的相关性主要取决于下面3个因素：

- 两个顶点之间的路径数；
- 两个顶点之间路径的长度；
- 两个顶点之间的路径经过的顶点。 

而相关性高的一对顶点一般具有如下特征： 

- 两个顶点之间有很多路径相连；
- 连接两个顶点之间的路径长度都比较短；
- 连接两个顶点之间的路径不会经过出度比较大的顶点。 

基于上面3个主要因素，本节将介绍一种基于随机游走的PersonalRank算法。 

假设要给用户u进行个性化推荐，可以从用户u对应的节点v~u~开始在用户物品二分图上进行随机游走。游走到任何一个节点时，首先按照概率α决定是继续游走，还是停止这次游走并从v~u~节点开始重新游走。如果决定继续游走，那么就从当前节点指向的节点中按照均匀分布随机选择一个节点作为游走下次经过的节点。这样，经过很多次随机游走后，每个物品节点被访问到的概率会收敛到一个数。最终的推荐列表中物品的权重就是物品节点的访问概率。 

![1560651273481](02-利用用户行为数据.assets\1560651273481.png)

