{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)# one_hot:把标签转换为只有0和1\n",
    "\n",
    "# 定义每个批次的大小\n",
    "## 可以通过修改批次来进行优化\n",
    "batch_size = 100 # 一次性放入一个批次 \n",
    "# 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size # 整除\n",
    "\n",
    "# 定义两个placeholder\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None,784]) # 28 * 28 转换为784\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# 创建一个简单的神经网络\n",
    "# W = tf.Variable(tf.zeros([784,10]))\n",
    "# b = tf.Variable(tf.zeros([10]))\n",
    "## 可以通过增加隐藏层进行优化 ## w和b的初始化值可以修改进行优化\n",
    "W1= tf.Variable(tf.truncated_normal([784,1000],stddev = 0.1)) # 截断的正态分布，stdde是标准差\n",
    "b1 = tf.Variable(tf.zeros([1000])+0.1)\n",
    "L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)\n",
    "L1_drop = tf.nn.dropout(L1,keep_prob) # keep_pron：表示百分之多少个神经元在工作\n",
    "\n",
    "## 隐藏层\n",
    "W2= tf.Variable(tf.truncated_normal([1000,500],stddev = 0.1)) # 截断的正态分布，stdde是标准差\n",
    "b2 = tf.Variable(tf.zeros([500])+0.1)\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)\n",
    "L2_drop = tf.nn.dropout(L2,keep_prob) # keep_pron：表示百分之多少个神经元在工作\n",
    "\n",
    "## 隐藏层\n",
    "W3= tf.Variable(tf.truncated_normal([500,100],stddev = 0.1)) # 截断的正态分布，stdde是标准差\n",
    "b3 = tf.Variable(tf.zeros([100])+0.1)\n",
    "L3 = tf.nn.tanh(tf.matmul(L2_drop,W3)+b3)\n",
    "L3_drop = tf.nn.dropout(L3,keep_prob) # keep_pron：表示百分之多少个神经元在工作\n",
    "\n",
    "W4= tf.Variable(tf.truncated_normal([100,10],stddev = 0.1)) # 截断的正态分布，stdde是标准差\n",
    "b4 = tf.Variable(tf.zeros([10])+0.1)\n",
    "prediction = tf.nn.softmax(tf.matmul(L3_drop,W4)+b4) ## 修改激活函数进行优化\n",
    "\n",
    "# 定义二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction)) ## 代价函数：交叉熵会不会更好\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "# 使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss) ## 学习率修改进行优化以及其他优化方式\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 结果存放在布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))# argmax 返回一维张量中最大的值所在的位置\n",
    "# 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) # 如果9个true，1个flase，那么就是9个1，1个0，准确率为90%\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # 把所有的图片训练21次\n",
    "    for epoch in range(31): ## 尝试训练更多的次数进行优化\n",
    "        for batch in range(n_batch): # 一共进行的批次（把训练集中所有的图片都循环了一次）\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)# 获得下一个一百张图片\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n",
    "            \n",
    "        test_acc = sess.run(accuracy,feed_dict={x: mnist.test.images,y: mnist.test.labels,keep_prob:1.0}) # 喂进去的就是测试集里面的图片和标签\n",
    "        train_acc = sess.run(accuracy,feed_dict={x: mnist.train.images,y: mnist.train.labels,keep_prob:1.0})# 用训练数据进行测试\n",
    "        print(\"Iter\" + str(epoch) + \",Testing Accuacy\" + str(test_acc)+\",Training Accuracy\"+ str(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iter0,Testing Accuacy0.9482,Training Accuracy0.95274544\n",
    "Iter1,Testing Accuacy0.9595,Training Accuracy0.9687091\n",
    "Iter2,Testing Accuacy0.9667,Training Accuracy0.97736365\n",
    "Iter3,Testing Accuacy0.9699,Training Accuracy0.98210907\n",
    "Iter4,Testing Accuacy0.9703,Training Accuracy0.9851636\n",
    "Iter5,Testing Accuacy0.9743,Training Accuracy0.9881455\n",
    "Iter6,Testing Accuacy0.9744,Training Accuracy0.98972726\n",
    "Iter7,Testing Accuacy0.9756,Training Accuracy0.9906\n",
    "Iter8,Testing Accuacy0.977,Training Accuracy0.9919818\n",
    "Iter9,Testing Accuacy0.9783,Training Accuracy0.9927273\n",
    "Iter10,Testing Accuacy0.9779,Training Accuracy0.9933818\n",
    "Iter11,Testing Accuacy0.978,Training Accuracy0.99372727\n",
    "Iter12,Testing Accuacy0.9781,Training Accuracy0.99407274\n",
    "Iter13,Testing Accuacy0.978,Training Accuracy0.9945273\n",
    "Iter14,Testing Accuacy0.9784,Training Accuracy0.9947818\n",
    "Iter15,Testing Accuacy0.9765,Training Accuracy0.9947636\n",
    "Iter16,Testing Accuacy0.9779,Training Accuracy0.9951636\n",
    "Iter17,Testing Accuacy0.9778,Training Accuracy0.9953455"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
